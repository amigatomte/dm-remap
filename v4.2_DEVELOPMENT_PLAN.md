# v4.2 Development Plan: Automatic Metadata Rebuild & Boot-Time Reassembly

**Created:** October 23, 2025  
**Target Completion:** October 26, 2025 (3 days)  
**Branch:** v4.2-auto-rebuild  
**Status:** Planning

## Objectives

1. **Automatic Metadata Rebuild**: Background repair and scrubbing to ensure metadata integrity
2. **Boot-Time Auto-Activation**: Automatically discover and reassemble dm-remap setups on system boot

## Current State Analysis

### What We Have ✅
- 5-copy redundant metadata storage
- Read function that selects best copy by sequence number
- Manual repair function (`dm_remap_repair_metadata_v4()`)
- Corruption detection during reads
- Repair statistics tracking

### What's Missing ❌
- Automatic repair scheduling (TODO on line 322)
- Background repair workqueue
- Periodic metadata health checks
- Async repair to avoid blocking I/O
- Sysfs interface for repair status and manual triggers
- **Boot-time automatic device discovery**
- **Automatic dm-remap target creation from discovered metadata**
- **udev integration for hotplug events**
- **systemd service for boot-time activation**

## Feature Breakdown

### Part A: Automatic Metadata Rebuild (6-7 hours)

### 1. Background Repair Infrastructure (2-3 hours)

**Files to modify:**
- `src/dm-remap-v4-real-main.c` - Add repair workqueue and work items
- `src/dm-remap-v4.h` - Add repair context structures

**Components:**
```c
struct dm_remap_repair_context {
    struct work_struct repair_work;
    struct delayed_work periodic_scrub_work;
    atomic_t repair_in_progress;
    atomic_t repairs_pending;
    atomic64_t last_repair_time;
    atomic64_t repairs_completed;
    atomic64_t scrubs_completed;
};
```

**Work functions:**
- `dm_remap_repair_work()` - Async metadata repair
- `dm_remap_periodic_scrub_work()` - Periodic health checks

### 2. Automatic Repair Scheduling (1-2 hours)

**Integration points:**
- Trigger repair when `dm_remap_read_metadata_v4()` detects corruption
- Use async work to avoid blocking the read path
- Respect device_active flag for clean shutdown

**Implementation:**
```c
// In dm_remap_read_metadata_v4() at line 322
if (valid_count < 5) {
    DMR_INFO("Metadata corruption detected: %d/5 copies valid, scheduling repair", 
             valid_count);
    dm_remap_schedule_metadata_repair(device);
}
```

### 3. Async Metadata Repair (2-3 hours)

**Key features:**
- Non-blocking repair using submit_bio async API (reuse v4.1 async infrastructure)
- Safe cancellation during device removal
- Retry logic for transient failures
- Progress tracking and logging

**Challenges:**
- Must handle concurrent I/O during repair
- Need to avoid repair loops if spare device is failing
- Proper error handling and recovery

### 4. Periodic Metadata Scrubbing (1-2 hours)

**Features:**
- Configurable scrub interval (default: 1 hour)
- Read and validate all 5 copies
- Automatic repair if corruption detected
- Low-priority background operation

**Implementation:**
```c
static void dm_remap_periodic_scrub_work(struct work_struct *work)
{
    // Check all 5 metadata copies
    // Repair if needed
    // Reschedule for next interval
}
```

### 5. Sysfs Interface (1-2 hours)

**New sysfs entries:**
```
/sys/block/dm-X/dm/remap/
├── metadata_health         (read: "5/5 copies valid")
├── metadata_repairs        (read: repair count)
├── last_repair_time        (read: timestamp)
├── trigger_repair          (write: "1" to force repair)
├── scrub_interval_seconds  (read/write: scrub interval)
└── auto_repair_enabled     (read/write: enable/disable)
```

### 6. Enhanced Async Metadata API (1 hour)

**Extend v4.1 async infrastructure:**
- Add repair-specific async operations
- Reuse existing completion handlers
- Add repair status to async context

### Part B: Boot-Time Auto-Activation (6-7 hours)

### 7. Module Initialization with Auto-Discovery (2 hours)

**Implement boot-time discovery:**
```c
static int dm_remap_init_v4_real(void)
{
    // ... existing init code ...
    
    // Schedule automatic discovery if enabled
    if (auto_activate_on_boot) {
        queue_delayed_work(system_wq, &boot_discovery_work, 
                          msecs_to_jiffies(5000)); // 5 sec delay
    }
}
```

**Module parameters:**
```c
static bool auto_activate_on_boot = true;
module_param(auto_activate_on_boot, bool, 0644);
MODULE_PARM_DESC(auto_activate_on_boot, 
                 "Automatically discover and activate setups on boot");

static unsigned int boot_discovery_delay = 5;
module_param(boot_discovery_delay, uint, 0644);
MODULE_PARM_DESC(boot_discovery_delay, 
                 "Delay in seconds before boot-time discovery");
```

### 8. Automatic Target Creation (2-3 hours)

**Implement dm-remap target auto-creation:**
```c
static void dm_remap_auto_activate_discovered_setups(struct work_struct *work)
{
    struct dm_remap_v4_discovery_result results[16];
    int num_results, i;
    
    // Scan all devices
    dm_remap_v4_scan_all_devices(...);
    
    // For each high-confidence discovery
    for (i = 0; i < num_results; i++) {
        if (results[i].confidence_score >= auto_activate_threshold) {
            dm_remap_create_target_from_metadata(&results[i]);
        }
    }
}
```

**Features:**
- Confidence threshold for auto-activation (default: 90%)
- Dry-run mode for testing
- Detailed logging of activation decisions
- Fallback to manual activation for low-confidence setups

### 9. udev Integration (1 hour)

**Create udev rules file:**
```udev
# /etc/udev/rules.d/99-dm-remap.rules
# Automatically trigger discovery when block devices appear

SUBSYSTEM=="block", ACTION=="add", \
  RUN+="/usr/local/bin/dm-remap-discover.sh $devnode"

SUBSYSTEM=="block", ACTION=="remove", \
  RUN+="/usr/local/bin/dm-remap-cleanup.sh $devnode"
```

**Helper script:**
```bash
#!/bin/bash
# /usr/local/bin/dm-remap-discover.sh
# Trigger discovery when new block device appears

DEVICE=$1
echo "scan" > /sys/kernel/dm-remap-v4/discovery/trigger_discovery
```

### 10. systemd Service (1 hour)

**Create systemd service:**
```ini
# /etc/systemd/system/dm-remap-discovery.service
[Unit]
Description=dm-remap Automatic Discovery Service
After=local-fs.target
Before=multi-user.target

[Service]
Type=oneshot
ExecStart=/usr/local/bin/dm-remap-boot-discover.sh
RemainAfterExit=yes

[Install]
WantedBy=multi-user.target
```

**Boot discovery script:**
```bash
#!/bin/bash
# /usr/local/bin/dm-remap-boot-discover.sh

# Wait for devices to settle
sleep 5

# Load dm-remap module if not loaded
modprobe dm_remap_v4_stats
modprobe dm_remap_v4_real

# Trigger discovery
echo "scan" > /sys/kernel/dm-remap-v4/discovery/trigger_discovery

# Check for auto-activation
AUTO_ACTIVATE=$(cat /sys/kernel/dm-remap-v4/discovery/auto_activate)
if [ "$AUTO_ACTIVATE" = "enabled" ]; then
    echo "1" > /sys/kernel/dm-remap-v4/discovery/trigger_auto_activate
fi
```

### 11. Enhanced Sysfs Interface (1 hour)

**New sysfs entries for auto-activation:**
```
/sys/kernel/dm-remap-v4/
├── discovery/
│   ├── auto_activate              (read/write: enable/disable)
│   ├── auto_activate_threshold    (read/write: confidence %)
│   ├── trigger_auto_activate      (write: "1" to activate)
│   ├── last_activation_time       (read: timestamp)
│   ├── activated_devices          (read: list)
│   └── activation_log             (read: recent activations)
```

## Implementation Timeline

### Day 1 (October 23, 2025) - 6 hours
- ✅ Create development plan
- ⏳ Create v4.2-auto-rebuild branch
- ⏳ Part A: Implement repair context structures
- ⏳ Part A: Add background repair workqueue
- ⏳ Part A: Implement basic repair work function
- ⏳ Part A: Integrate repair scheduling into read path

### Day 2 (October 24, 2025) - 8 hours
- ⏳ Part A: Implement async metadata repair
- ⏳ Part A: Add periodic scrubbing
- ⏳ Part A: Implement sysfs interface for repair
- ⏳ Part B: Module init with auto-discovery
- ⏳ Part B: Implement automatic target creation
- ⏳ Part B: Add module parameters

### Day 3 (October 25, 2025) - 6 hours
- ⏳ Part B: Create udev rules
- ⏳ Part B: Create systemd service
- ⏳ Part B: Enhanced sysfs for auto-activation
- ⏳ Testing both parts A and B
- ⏳ Integration testing

### Day 4 (October 26, 2025) - 4 hours
- ⏳ Edge case testing (device removal during repair, boot failures, etc.)
- ⏳ Documentation (installation guide, troubleshooting)
- ⏳ Merge to main and tag v4.2

## Technical Challenges

### Challenge 1: Concurrent I/O During Repair
**Problem:** User I/O may be happening while repair is running  
**Solution:** Use async I/O, don't block main I/O path, check device_active

### Challenge 2: Repair Loops on Failing Device
**Problem:** If spare device is failing, repair may keep detecting corruption  
**Solution:** 
- Exponential backoff on repair failures
- Max retry limit (e.g., 3 attempts per hour)
- Mark device as degraded if repairs keep failing

### Challenge 3: Clean Shutdown During Repair
**Problem:** Device removal while repair is in progress  
**Solution:** Reuse v4.1 cancellation infrastructure with complete_all()

### Challenge 4: Avoiding Deadlocks
**Problem:** Repair takes mutex, reads/writes metadata  
**Solution:** 
- Use separate repair workqueue (not metadata_workqueue)
- Release mutex before async I/O
- Follow v4.1 patterns

### Challenge 5: Boot-Time Race Conditions
**Problem:** Devices may not be ready when module loads  
**Solution:**
- Delay discovery by configurable amount (default: 5 seconds)
- Retry discovery if devices not found
- Listen for udev events for late-arriving devices

### Challenge 6: Auto-Activation Safety
**Problem:** Wrong device could be activated (UUID collision, etc.)  
**Solution:**
- Require high confidence threshold (90%+)
- Validate device fingerprints
- Log all activation decisions
- Provide dry-run mode for testing

### Challenge 7: Multiple Reboots
**Problem:** System reboots multiple times, re-activating same setups  
**Solution:**
- Track activated UUIDs to prevent duplicates
- Check if dm-remap device already exists
- Persistent state file to remember activations

## Success Criteria

### Part A: Metadata Rebuild
- ✅ Automatic repair triggered when corruption detected
- ✅ Periodic scrubbing runs without blocking I/O
- ✅ Sysfs interface works for manual triggers
- ✅ Clean device removal during repair (no hangs)
- ✅ Repairs complete successfully on test corruption
- ✅ No performance impact on normal I/O
- ✅ Comprehensive logging of repair activity

### Part B: Boot-Time Auto-Activation
- ✅ Module loads and discovers devices automatically on boot
- ✅ High-confidence setups are auto-activated
- ✅ Low-confidence setups logged but not activated
- ✅ No duplicate activations on multiple reboots
- ✅ udev integration works for hotplug events
- ✅ systemd service starts and completes successfully
- ✅ Manual override via module parameters works
- ✅ Activation log provides audit trail

## Testing Plan

### Unit Tests - Part A (Metadata Rebuild)
1. Corrupt 1 copy, verify auto-repair
2. Corrupt 4 copies, verify repair from good copy
3. Periodic scrub detects corruption
4. Manual repair trigger works
5. Device removal during repair (no hang)
6. Concurrent I/O during repair

### Unit Tests - Part B (Auto-Activation)
1. Module init triggers discovery
2. High-confidence setup auto-activates
3. Low-confidence setup skipped
4. UUID collision detected and handled
5. Already-active device skipped
6. Module parameter override works

### Integration Tests
1. Full metadata corruption scenario with auto-repair
2. Repair under I/O load
3. Multiple devices with repairs
4. Long-running stability test
5. **Boot-time activation end-to-end**
6. **Reboot test: devices persist across reboots**
7. **Hotplug test: new device triggers discovery**
8. **Multi-setup test: multiple dm-remap setups activated**

### Edge Cases
1. All 5 metadata copies corrupted (should fail gracefully)
2. Repair during heavy I/O
3. Spare device becomes unavailable during repair
4. Multiple rapid corruption events
5. **Boot with no valid metadata (should not activate)**
6. **Boot with corrupted metadata (should repair then activate)**
7. **Device appears after boot (udev should trigger)**
8. **System shutdown during activation**

## Deliverables

1. **Code - Part A (Metadata Rebuild)**
   - Background repair infrastructure
   - Async repair implementation
   - Periodic scrubbing
   - Sysfs interface for repair

2. **Code - Part B (Auto-Activation)**
   - Module init with auto-discovery
   - Automatic target creation
   - Module parameters
   - Enhanced sysfs interface

3. **System Integration**
   - udev rules file
   - systemd service unit
   - Installation scripts
   - Helper scripts

4. **Documentation**
   - API documentation
   - Sysfs interface guide
   - Installation guide
   - Boot-time activation guide
   - Troubleshooting guide
   - Architecture diagrams

5. **Tests**
   - Unit test suite (Parts A & B)
   - Integration test scripts
   - Boot test automation
   - Corruption injection tools

6. **Release**
   - v4.2 git tag
   - Release notes
   - Updated roadmap
   - Migration guide from manual to auto-activation

## Risk Assessment

| Risk | Severity | Mitigation |
|------|----------|------------|
| Deadlocks during repair | HIGH | Follow v4.1 async patterns carefully |
| Performance impact | MEDIUM | Use low-priority workqueues, async I/O |
| Repair loops | MEDIUM | Exponential backoff, retry limits |
| Boot activation failures | HIGH | High confidence threshold, validation, dry-run mode |
| Wrong device activation | HIGH | UUID validation, fingerprinting, audit logging |
| Activation race conditions | MEDIUM | Delayed discovery, retry logic, state tracking |
| udev/systemd integration issues | MEDIUM | Extensive testing, fallback to manual activation |
| Complexity | MEDIUM | Build on proven v4.1 infrastructure |

## Dependencies

- ✅ v4.1 async metadata I/O infrastructure
- ✅ v4.0 5-copy redundant storage
- ✅ Existing repair function
- ✅ v4.0 setup reassembly discovery system
- ✅ v4.0 device fingerprinting
- ✅ v4.0 metadata storage format
- ⚠️ systemd (for service integration)
- ⚠️ udev (for hotplug integration)

## Next Steps

1. Create v4.2-auto-rebuild branch
2. **Part A First:** Start with repair context structures and basic repair
3. Complete Part A (metadata rebuild) and test thoroughly
4. **Then Part B:** Implement auto-activation on proven repair foundation
5. Integration testing of both parts together
6. System integration (udev, systemd)
7. Documentation and release

---

**Ready to start implementation!** 🚀

**Note:** This expanded scope combines two critical features that work together:
- **Part A** ensures metadata is always healthy and recoverable
- **Part B** uses that healthy metadata to automatically rebuild setups

Together, they provide a complete "set it and forget it" solution for dm-remap deployments!
